"""
Script para comparar resultados dos prompts:
- V1 (Baseline)
- V2.8 (Scalpel - Otimizado)
- V3 (User Provided)

Salva resultados em Markdown e JSON.
"""

import sys
import json
import argparse
from evaluate import run_evaluation_for_prompt
from dotenv import load_dotenv

load_dotenv()

def run_comparison(model_name: str = "gemini-2.0-flash"):
    prompts_to_compare = [
        ("Baseline (v1)", "bug_to_user_story_v1"),
        ("Final (v2 XML)", "bug_to_user_story_v2")
    ]
    
    results = {}
    
    print(f"üèÅ Iniciando Compara√ß√£o Geral com Modelo: {model_name} ...")
    
    for label, prompt_name in prompts_to_compare:
        print(f"\n‚û°Ô∏è  Avaliando {label} [{prompt_name}]...")
        scores = run_evaluation_for_prompt(prompt_name, model_name=model_name)
        if scores:
            results[label] = scores
        else:
            print(f"‚ö†Ô∏è  Sem resultados para {label}")

    if not results:
        print("‚ùå Nenhuma avalia√ß√£o obteve sucesso.")
        return

    # Montar Tabela
    # Identificar todas as m√©tricas presentes
    all_metrics = set()
    for res in results.values():
        all_metrics.update(res.keys())
    
    metrics = sorted(list(all_metrics))
    
    # Cabe√ßalho
    labels = [p[0] for p in prompts_to_compare if p[0] in results]
    header = f"| M√©trica | {' | '.join(labels)} | Meta (>0.9) |"
    separator = f"|---|{'---|' * len(labels)}---|"
    
    match_rows = []
    
    for metric in metrics:
        row = f"| {metric} |"
        for label in labels:
            score = results[label].get(metric, 0.0)
            status = "‚úÖ" if score >= 0.9 else "‚ùå"
            # Highlight best score
            row += f" {score:.4f} |"
        
        row += " 0.9 |"
        match_rows.append(row)

    # Calcular M√©dias
    avg_row = f"| **M√âDIA GERAL** |"
    for label in labels:
        scores = results[label].values()
        avg = sum(scores) / len(scores) if scores else 0
        avg_row += f" **{avg:.4f}** |"
    avg_row += " - |"

    md_content = f"# Relat√≥rio de Compara√ß√£o de Prompts\n\n"
    md_content += f"{header}\n{separator}\n"
    md_content += "\n".join(match_rows)
    md_content += f"\n{avg_row}\n"
    
    # An√°lise R√°pida
    md_content += "\n## üèÜ An√°lise R√°pida\n"
    
    # M√©tricas alvo
    target_metrics = ["tone", "acceptance_criteria", "user_story_format", "completeness"]
    
    def check_success(metrics_dict):
        return all(metrics_dict.get(m, 0) >= 0.9 for m in target_metrics)
        
    v2_success = check_success(results.get("Final (v2 XML)", {}))
    
    if v2_success:
        md_content += "\n- **V2 (XML)**: APROVADO! Todos os crit√©rios (Tone, AC, Format, Completeness) est√£o >= 0.9."
    else:
        md_content += "\n- **Aten√ß√£o**: O prompt V2 n√£o atingiu todas as metas. Verifique os detalhes."

    filename = f"comparison_report_{model_name.replace(':', '').replace('.', '-')}.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(md_content)
        
    print(f"\n‚úÖ Relat√≥rio salvo em {filename}")
    print(md_content)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="gemini-2.0-flash")
    args = parser.parse_args()
    
    run_comparison(model_name=args.model)
