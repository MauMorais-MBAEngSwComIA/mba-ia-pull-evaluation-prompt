"""
FunÃ§Ãµes auxiliares para o projeto de otimizaÃ§Ã£o de prompts.
"""

import os
import yaml
import json
from typing import Dict, Any, Optional
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()


def load_yaml(file_path: str) -> Optional[Dict[str, Any]]:
    """
    Carrega arquivo YAML.

    Args:
        file_path: Caminho do arquivo YAML

    Returns:
        DicionÃ¡rio com conteÃºdo do YAML ou None se erro
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        return data
    except FileNotFoundError:
        print(f"âŒ Arquivo nÃ£o encontrado: {file_path}")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ Erro ao parsear YAML: {e}")
        return None
    except Exception as e:
        print(f"âŒ Erro ao carregar arquivo: {e}")
        return None


def save_yaml(data: Dict[str, Any], file_path: str) -> bool:
    """
    Salva dados em arquivo YAML.

    Args:
        data: Dados para salvar
        file_path: Caminho do arquivo de saÃ­da

    Returns:
        True se sucesso, False caso contrÃ¡rio
    """
    try:
        output_file = Path(file_path)
        # Garantir que o diretÃ³rio pai exista
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(data, f, allow_unicode=True, sort_keys=False, indent=2)

        return True
    except Exception as e:
        print(f"âŒ Erro ao salvar arquivo: {e}")
        return False


def check_env_vars(required_vars: list) -> bool:
    """
    Verifica se variÃ¡veis de ambiente obrigatÃ³rias estÃ£o configuradas.

    Args:
        required_vars: Lista de variÃ¡veis obrigatÃ³rias

    Returns:
        True se todas configuradas, False caso contrÃ¡rio
    """
    missing_vars = []

    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)

    if missing_vars:
        print("âŒ VariÃ¡veis de ambiente faltando:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\nConfigure-as no arquivo .env antes de continuar.")
        return False

    return True


def format_score(score: float, threshold: float = 0.9) -> str:
    """
    Formata score com indicador visual de aprovaÃ§Ã£o.

    Args:
        score: Score entre 0.0 e 1.0
        threshold: Limite mÃ­nimo para aprovaÃ§Ã£o

    Returns:
        String formatada com score e sÃ­mbolo
    """
    symbol = "âœ“" if score >= threshold else "âœ—"
    return f"{score:.2f} {symbol}"


def print_section_header(title: str, char: str = "=", width: int = 50):
    """
    Imprime cabeÃ§alho de seÃ§Ã£o formatado.

    Args:
        title: TÃ­tulo da seÃ§Ã£o
        char: Caractere para a linha
        width: Largura da linha
    """
    print("\n" + char * width)
    print(title)
    print(char * width + "\n")


def extract_json_from_response(response_text: str) -> Optional[Dict[str, Any]]:
    """
    Extrai JSON de uma resposta de LLM que pode conter texto adicional.

    Args:
        response_text: Texto da resposta do LLM

    Returns:
        DicionÃ¡rio extraÃ­do ou None se nÃ£o encontrar JSON vÃ¡lido
    """
    try:
        return json.loads(response_text)
    except json.JSONDecodeError:
        # Tentar encontrar JSON no meio do texto
        # Procura pelo primeiro '{' e pelo Ãºltimo '}'
        start = response_text.find('{')
        end = response_text.rfind('}') + 1

        if start != -1 and end > start:
            try:
                json_str = response_text[start:end]
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass
                
        # Tentar consertar JSONs mal formatados comuns (ex: markdown code blocks)
        if "```json" in response_text:
            try:
                json_str = response_text.split("```json")[1].split("```")[0].strip()
                return json.loads(json_str)
            except:
                pass

    return None


def get_llm(model: Optional[str] = None, temperature: float = 0.0):
    """
    Retorna uma instÃ¢ncia de LLM configurada baseada no provider.
    Prioriza configuraÃ§Ã£o do .env ou defaults do Google GenAI.
    Retorna uma instÃ¢ncia de LLM com fallback automÃ¡tico de providers.
    Ordem de preferÃªncia:
    1. Google (GEMINI_API_KEY ou GOOGLE_API_KEY)
    2. OpenAI (OPENAI_API_KEY)
    """
    # 1. Tentar Google Gemini primeiro (PreferÃªncia do User)
    google_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
    openai_key = os.getenv('OPENAI_API_KEY')
    
    # Se o usuÃ¡rio forÃ§ou um provider via env, respeitar (mas cair no fallback se falhar setup?)
    # A regra do usuÃ¡rio foi: "Se a chave da Google estiver configurada, ela Ã© a default."
    
    if google_key:
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            default_model = 'gemini-2.0-flash'
            model_name = model or os.getenv('LLM_MODEL', default_model)
            
            # Ajuste para garantir que nÃ£o estamos usando modelo OpenAI com Google
            if "gpt" in model_name:
                model_name = default_model

            print(f"ðŸ¤– Usando Provider: Google | Modelo: {model_name}")
            return ChatGoogleGenerativeAI(
                model=model_name,
                temperature=temperature,
                google_api_key=google_key
            )
        except ImportError:
            print("âš ï¸  Biblioteca langchain-google-genai nÃ£o encontrada.")
            pass # Tentar OpenAI

    # 2. Se nÃ£o tem Google ou falhou, tentar OpenAI
    if openai_key:
        try:
            from langchain_openai import ChatOpenAI
            default_model = 'gpt-4o'
            model_name = model or os.getenv('LLM_MODEL', default_model)

            # Ajuste para garantir que nÃ£o estamos usando modelo Gemini com OpenAI
            if "gemini" in model_name:
                model_name = default_model

            print(f"ðŸ¤– Usando Provider: OpenAI | Modelo: {model_name}")
            return ChatOpenAI(
                model=model_name,
                temperature=temperature,
                api_key=openai_key
            )
        except ImportError:
            print("âš ï¸  Biblioteca langchain-openai nÃ£o encontrada.")
            pass

    # 3. Se chegou aqui, nÃ£o tem chaves configuradas
    raise ValueError(
        "âŒ Nenhuma chave de API encontrada!\n"
        "Configure no .env uma das opÃ§Ãµes:\n"
        "   - GOOGLE_API_KEY (Recomendado: gemini-2.0-flash)\n"
        "   - OPENAI_API_KEY (Fallback: gpt-4o)"
    )


def get_eval_llm(model: Optional[str] = None, temperature: float = 0.0):
    """
    Retorna LLM auto-configurado para avaliaÃ§Ã£o.
    """
    # Para avaliaÃ§Ã£o, geralmente queremos modelos mais robustos.
    # Se estivermos no Google -> gemini-2.0-flash (ou o solicitado)
    # Se OpenAI -> gpt-4o
    return get_llm(model=model, temperature=temperature)
