# **Pull, Otimização e Avaliação de Prompts com LangChain e LangSmith**

## **Objetivo**

O software entregue deve ser capaz de:

* **Fazer pull de prompts** do LangSmith Prompt Hub contendo prompts de baixa qualidade.
* **Refatorar e otimizar** esses prompts usando técnicas avançadas de Prompt Engineering.
* **Fazer push dos prompts otimizados** de volta ao LangSmith.
* **Avaliar a qualidade** através de métricas customizadas (F1-Score, Clarity, Precision).
* **Atingir pontuação mínima de 0.9 (90%)** em todas as métricas de avaliação.

---

## **Exemplo no CLI**

Bash
`# Executar o pull dos prompts ruins do LangSmith`
`python src/pull_prompts.py`

`# Executar avaliação inicial (prompts ruins)`
`python src/evaluate.py`

**Resultado da Avaliação Inicial:**

Prompt: support\_bot\_v1a

* Helpfulness: 0.45 | Correctness: 0.52 | F1-Score: 0.48 | Clarity: 0.50 | Precision: 0.46
* **Status: FALHOU** \- Métricas abaixo do mínimo de 0.9

Bash
`# Após refatorar os prompts e fazer push`
`python src/push_prompts.py`

`# Executar avaliação final (prompts otimizados)`
`python src/evaluate.py`

**Resultado da Avaliação Final:**

Prompt: support\_bot\_v2\_optimized

* Helpfulness: 0.94 | Correctness: 0.96 | F1-Score: 0.93 | Clarity: 0.95 | Precision: 0.92
* **Status: APROVADO ✓** \- Todas as métricas atingiram o mínimo de 0.9

---

## **Stack Tecnológica**

### **Tecnologias Obrigatórias**

* **Linguagem:** Python 3.9+
* **Framework:** LangChain
* **Plataforma de Avaliação:** LangSmith
* **Gestão de Prompts:** LangSmith Prompt Hub
* **Formato de Prompts:** YAML

### **Modelos de LLM Sugeridos**

* **OpenAI:** gpt-4o-mini (Resposta) e gpt-4o (Avaliação).
* **Gemini:** gemini-2.0-flash (Resposta e Avaliação).

---

## **Requisitos do Desafio**

### **1\. Pull do Prompt Inicial**

O repositório base contém prompts de baixa qualidade.

* **Tarefa:** Configurar credenciais no .env e executar src/pull\_prompts.py.
* **Alvo:** leonanluppi/bug\_to\_user\_story\_v1.
* **Saída:** prompts/raw\_prompts.yml.

### **2\. Otimização do Prompt**

Refatorar o prompt em prompts/bug\_to\_user\_story\_v2.yml aplicando pelo menos **duas** técnicas:

* Few-shot Learning
* Chain of Thought (CoT)
* Tree of Thought / Skeleton of Thought
* ReAct ou Role Prompting

**O prompt otimizado deve conter:** Instruções claras, regras explícitas, exemplos de entrada/saída (Few-shot), tratamento de *edge cases* e separação entre System e User Prompt.

### **3\. Push e Avaliação**

* **Push:** Criar src/push\_prompts.py para enviar a versão v2 ao seu repositório no LangSmith.
* **Iteração:** Realizar de 3 a 5 iterações até que as métricas (**Tone, Acceptance Criteria, User Story Format e Completeness**) sejam todas $\\ge 0.9$.

### **4\. Testes de Validação**

Implementar testes em tests/test\_prompts.py usando **pytest** para garantir:

1. Existência de System Prompt.
2. Definição de Persona (Role).
3. Exigência de formato (Markdown/User Story).
4. Presença de Few-shot examples.
5. Ausência de termos "\[TODO\]".
6. Uso de pelo menos 2 técnicas avançadas.

---

## **Estrutura do Projeto**

Plaintext
`desafio-prompt-engineer/`
`├── prompts/`
`│   ├── bug_to_user_story_v1.yml  # Original`
`│   └── bug_to_user_story_v2.yml  # Otimizado`
`├── src/`
`│   ├── pull_prompts.py           # Script de captura`
`│   ├── push_prompts.py           # Script de publicação`
`│   ├── evaluate.py               # Motor de avaliação`
`│   └── metrics.py                # Lógica das métricas`
`└── tests/`
    `└── test_prompts.py           # Testes automatizados`

---

## **Critérios de Entrega (README.md)**

O arquivo de documentação deve conter obrigatoriamente:

* **Técnicas Aplicadas:** Justificativa e exemplos práticos.
* **Resultados Finais:** Link do dashboard LangSmith, screenshots e tabela comparativa v1 vs v2.
* **Instruções de Execução:** Como configurar o ambiente e rodar os scripts.
