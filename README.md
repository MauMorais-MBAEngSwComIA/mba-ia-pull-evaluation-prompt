# Prompt Evaluation: Bug to User Story

Este projeto implementa um pipeline completo de Engenharia de Prompt para converter **Bug Reports** em **User Stories** no padrÃ£o INVEST, utilizando **LangChain** e **LangSmith** para avaliaÃ§Ã£o contÃ­nua.

## ðŸŽ¯ Objetivo
Transformar relatos de bugs tÃ©cnicos (muitas vezes curtos ou confusos) em User Stories estruturadas, com critÃ©rios de aceite claros e contexto tÃ©cnico preservado.

## ðŸ› ï¸ Stack TecnolÃ³gico
- **Linguagem**: Python 3.9+
- **OrquestraÃ§Ã£o**: LangChain
- **Observabilidade**: LangSmith
- **LLM (Generator & Evaluator)**: Google Gemini 2.5 Flash
- **MÃ©tricas**: Customizadas (Tone, Acceptance Criteria, User Story Format, Completeness)

## ðŸ”„ TÃ©cnicas de Prompt Engineering Aplicadas

O prompt V2 utiliza **4 tÃ©cnicas avanÃ§adas** combinadas para maximizar a qualidade:

### 1. Role Prompting (Persona)
O prompt define uma persona especializada de **Product Owner SÃªnior + Analista de QA** com 15 anos de experiÃªncia. Isso direciona o LLM a adotar um vocabulÃ¡rio tÃ©cnico adequado e a priorizar valor de negÃ³cio sobre detalhes de implementaÃ§Ã£o.

> **Por quÃª?** Sem persona, o modelo tende a gerar User Stories genÃ©ricas e com tom de chatbot. Com a persona, o tom profissional atingiu **1.00** (nota mÃ¡xima).

### 2. Chain of Thought (CoT)
InstruÃ§Ãµes explÃ­citas de raciocÃ­nio passo a passo: AnÃ¡lise do Problema â†’ ExtraÃ§Ã£o de Dados â†’ DefiniÃ§Ã£o de Valor â†’ Mapeamento de CenÃ¡rios. Isso forÃ§a o modelo a "pensar antes de escrever".

> **Por quÃª?** O CoT evita que o modelo pule diretamente para a resposta, reduzindo alucinaÃ§Ãµes e melhorando a completude dos cenÃ¡rios gerados.

### 3. Few-Shot Learning
Um exemplo completo de entrada (Bug Report) e saÃ­da esperada (User Story com CritÃ©rios de Aceite Gherkin) Ã© fornecido dentro do prompt.

> **Por quÃª?** O exemplo serve como "molde" para o formato, garantindo que o modelo reproduza a estrutura Markdown com `# TÃ­tulo`, `**Como**/**Eu quero**/**Para que**` e `Dado/Quando/EntÃ£o`.

### 4. XML Isolation (EstruturaÃ§Ã£o de SaÃ­da)
O raciocÃ­nio do modelo Ã© encapsulado em tags `<thinking>` e a User Story final em `<user_story>`. Apenas o conteÃºdo dentro de `<user_story>` Ã© avaliado.

> **Por quÃª?** Modelos mais recentes (como o Gemini 2.5 Flash) tendem a "conversar" antes de responder. O XML separa pensamento de entrega, permitindo que o avaliador ignore o raciocÃ­nio e avalie apenas o produto final.

## ðŸ“Š Resultados da AvaliaÃ§Ã£o (V1 vs V2)

O pipeline utiliza o **LLM-as-a-Judge** (Gemini 2.5 Flash) para avaliar as stories geradas contra um dataset de 10 bug reports de referÃªncia.

| MÃ©trica | V1 (Baseline) | **V2 (Optimized)** | Meta (>0.9) |
|---|---|---|---|
| **Tone** | 0.99 | **1.00** | âœ… |
| **Completeness** | 0.99 | **0.99** | âœ… |
| **User Story Format** | 0.97 | **0.99** | âœ… |
| **Acceptance Criteria** | 0.96 | **0.97** | âœ… |

### ðŸ”— Dashboard LangSmith



- **Experimento V1 (Baseline)**: [Ver no LangSmith](https://smith.langchain.com/o/4edf22f3-ecb6-499b-b514-311998f18731/datasets/1d2bae30-bdaa-4f4f-8827-5c29301afbf6/compare?selectedSessions=6b391136-c76e-44f4-96fd-4030d084eb4c)
- **Experimento V2 (Otimizado)**: [Ver no LangSmith](https://smith.langchain.com/o/4edf22f3-ecb6-499b-b514-311998f18731/datasets/1d2bae30-bdaa-4f4f-8827-5c29301afbf6/compare?selectedSessions=e45a9d2e-4729-4080-9b94-6425cfd6b1a9)
- **ComparaÃ§Ã£o V1 vs V2**: [Ver no LangSmith](https://smith.langchain.com/o/4edf22f3-ecb6-499b-b514-311998f18731/datasets/1d2bae30-bdaa-4f4f-8827-5c29301afbf6/compare?selectedSessions=e45a9d2e-4729-4080-9b94-6425cfd6b1a9%2C6b391136-c76e-44f4-96fd-4030d084eb4c&source=e45a9d2e-4729-4080-9b94-6425cfd6b1a9)

### ðŸ“¸ Screenshots

![Resultado do Experimento V2 no LangSmith](docs/screenshots/experiment_v2.png)

![ComparaÃ§Ã£o V1 vs V2 no LangSmith](docs/screenshots/comparison_v1_v2.png)

## ðŸš€ Como Executar

### 1. InstalaÃ§Ã£o
```bash
pip install -r requirements.txt
```

### 2. ConfiguraÃ§Ã£o
Crie um arquivo `.env` baseado no `.env.example`:
```bash
cp .env.example .env
# Edite o .env com suas chaves de API
```

### 3. Pull do Prompt Original
```bash
python src/pull_prompts.py
```

### 4. Push do Prompt Otimizado
```bash
python src/push_prompts.py
```

### 5. Rodar AvaliaÃ§Ã£o

Avaliar o prompt otimizado (V2):
```bash
python src/evaluate.py --model gemini-2.5-flash
```

Comparar V1 vs V2:
```bash
python src/compare_prompts.py --model gemini-2.5-flash
```

### 6. Testes UnitÃ¡rios
```bash
pytest tests/
```

## ðŸ“‚ Estrutura do Projeto

```
prompt-evaluation-langchain-langsmith/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ bug_to_user_story_v1.yml      # Prompt Original (Baseline)
â”‚   â””â”€â”€ bug_to_user_story_v2.yml      # Prompt Otimizado (Final)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pull_prompts.py               # Script de captura do LangSmith Hub
â”‚   â”œâ”€â”€ push_prompts.py               # Script de publicaÃ§Ã£o no LangSmith Hub
â”‚   â”œâ”€â”€ evaluate.py                   # Motor de avaliaÃ§Ã£o (LLM-as-Judge)
â”‚   â”œâ”€â”€ compare_prompts.py            # ComparaÃ§Ã£o V1 vs V2
â”‚   â”œâ”€â”€ metrics.py                    # LÃ³gica das mÃ©tricas customizadas
â”‚   â””â”€â”€ utils.py                      # UtilitÃ¡rios e configuraÃ§Ã£o de LLM
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ bug_to_user_story.jsonl       # Dataset de avaliaÃ§Ã£o (10 exemplos)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_prompts.py               # Testes automatizados (24 testes)
â”œâ”€â”€ docs/                             # DocumentaÃ§Ã£o do desafio
â”œâ”€â”€ .env.example                      # Template de configuraÃ§Ã£o
â”œâ”€â”€ requirements.txt                  # DependÃªncias Python
â””â”€â”€ README.md                         # Este arquivo
```

---

## ðŸ“‹ RelatÃ³rios Gerados

- [**RelatÃ³rio de ComparaÃ§Ã£o V1 vs V2**](comparison_report.md) â€” Resultado da avaliaÃ§Ã£o lado a lado dos prompts.
- [**RelatÃ³rio de Testes**](test_report.md) â€” Resultado do `pytest` com os 6 critÃ©rios de validaÃ§Ã£o. *(Regenerado automaticamente a cada execuÃ§Ã£o)*
- [**RelatÃ³rio de Compliance**](compliance_report.md) â€” VerificaÃ§Ã£o de conformidade com todos os requisitos do desafio.

---
**Desenvolvido como parte do Desafio TÃ©cnico de Prompt Engineering.**
